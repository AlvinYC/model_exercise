{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code ==> https://graviraja.github.io/seqtoseqimp/#\n",
    "# dataset ==> https://github.com/multi30k/dataset\n",
    "import os\n",
    "import math\n",
    "import random\n",
    "import spacy\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "from torchtext.datasets import TranslationDataset, Multi30k\n",
    "from torchtext.data import Field, BucketIterator\n",
    "from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SEED = 5\n",
    "random.seed(SEED)\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for tokenizing the english sentences\n",
    "spacy_en = spacy.load('en')\n",
    "# for tokenizing the german sentences\n",
    "spacy_de = spacy.load('de')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize_de(text):\n",
    "    # tokenizes the german text into a list of strings(tokens) and reverse it\n",
    "    # we are reversing the input sentences, as it is observed \n",
    "    # by reversing the inputs we will get better results\n",
    "    return [tok.text for tok in spacy_de.tokenizer(text)][::-1]     # list[::-1] used to reverse the list\n",
    "\n",
    "\n",
    "def tokenize_en(text):\n",
    "    # tokenizes the english text into a list of strings(tokens)\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "SRC = Field(tokenize=tokenize_de, init_token='<sos>', eos_token='<eos>', lower=True)\n",
    "TRG = Field(tokenize=tokenize_en, init_token='<sos>', eos_token='<eos>', lower=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data...\n"
     ]
    }
   ],
   "source": [
    "train_data, valid_data, test_data = Multi30k.splits(exts=('.de', '.en'), fields=(SRC, TRG))\n",
    "print('Loaded data...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training examples: 29000\n",
      "Number of validation examples: 1014\n",
      "Number of testing examples: 1000\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of training examples: {len(train_data.examples)}\")\n",
    "print(f\"Number of validation examples: {len(valid_data.examples)}\")\n",
    "print(f\"Number of testing examples: {len(test_data.examples)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "src: ['.', 'büsche', 'vieler', 'nähe', 'der', 'in', 'freien', 'im', 'sind', 'männer', 'weiße', 'junge', 'zwei']\n",
      "trg: ['two', 'young', ',', 'white', 'males', 'are', 'outside', 'near', 'many', 'bushes', '.']\n"
     ]
    }
   ],
   "source": [
    "print(f\"src: {vars(train_data.examples[0])['src']}\")\n",
    "print(f\"trg: {vars(train_data.examples[0])['trg']}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocab builded...\n"
     ]
    }
   ],
   "source": [
    "# build the vocab\n",
    "# consider words which are having atleast min_freq.\n",
    "# words having less than min_freq will be replaced by <unk> token\n",
    "SRC.build_vocab(train_data, min_freq=2)\n",
    "TRG.build_vocab(train_data, min_freq=2)\n",
    "print('Vocab builded...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unique tokens in source (de) vocabulary: 7855\n",
      "Unique tokens in target (en) vocabulary: 5893\n"
     ]
    }
   ],
   "source": [
    "print(f\"Unique tokens in source (de) vocabulary: {len(SRC.vocab)}\")\n",
    "print(f\"Unique tokens in target (en) vocabulary: {len(TRG.vocab)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define batch size\n",
    "BATCH_SIZE = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use gpu if available, else use cpu\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# create data iterators for the data\n",
    "# padding all the sentences to same length, replacing words by its index,\n",
    "# bucketing (minimizes the amount of padding by grouping similar length sentences)\n",
    "train_iterator, valid_iterator, test_iterator = BucketIterator.splits((train_data, valid_data, test_data), batch_size=BATCH_SIZE, device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cuda')"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' Sequence to sequence networks consists of Encoder and Decoder modules.\n",
    "    This class contains the implementation of Encoder module.\n",
    "    Args:\n",
    "        input_dim: A integer indicating the size of input dimension.\n",
    "        emb_dim: A integer indicating the size of embeddings.\n",
    "        hidden_dim: A integer indicating the hidden dimension of RNN layers.\n",
    "        n_layers: A integer indicating the number of layers.\n",
    "        dropout: A float indicating dropout.\n",
    "    '''\n",
    "    def __init__(self, input_dim, emb_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "        self.input_dim = input_dim\n",
    "        self.emb_dim = emb_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(input_dim, emb_dim)\n",
    "        self.rnn = nn.LSTM(emb_dim, hidden_dim, n_layers, dropout=dropout)  # default is time major\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src):\n",
    "        # src is of shape [sentence_length, batch_size], it is time major\n",
    "\n",
    "        # embedded is of shape [sentence_length, batch_size, embedding_size]\n",
    "        embedded = self.embedding(src)\n",
    "        embedded = self.dropout(embedded)\n",
    "\n",
    "        # inputs to the rnn is input, (h, c); if hidden, cell states are not passed means default initializes to zero.\n",
    "        # input is of shape [sequence_length, batch_size, input_size]\n",
    "        # hidden is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [num_layers * num_directions, batch_size, hidden_size]\n",
    "        outputs, (hidden, cell) = self.rnn(embedded)\n",
    "\n",
    "        # outputs are always from the top hidden layer, if bidirectional outputs are concatenated.\n",
    "        # outputs shape [sequence_length, batch_size, hidden_dim * num_directions]\n",
    "        return hidden, cell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    ''' This class contains the implementation of Decoder Module.\n",
    "    Args:\n",
    "        embedding_dim: A integer indicating the embedding size.\n",
    "        output_dim: A integer indicating the size of output dimension.\n",
    "        hidden_dim: A integer indicating the hidden size of rnn.\n",
    "        n_layers: A integer indicating the number of layers in rnn.\n",
    "        dropout: A float indicating the dropout.\n",
    "    '''\n",
    "    def __init__(self, embedding_dim, output_dim, hidden_dim, n_layers, dropout):\n",
    "        super().__init__()\n",
    "\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.output_dim = output_dim\n",
    "        self.hidden_dim = hidden_dim\n",
    "        self.n_layers = n_layers\n",
    "        self.dropout = dropout\n",
    "\n",
    "        self.embedding = nn.Embedding(output_dim, embedding_dim)\n",
    "        self.rnn = nn.LSTM(embedding_dim, hidden_dim, n_layers, dropout=dropout)\n",
    "        self.linear = nn.Linear(hidden_dim, output_dim)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input, hidden, cell):\n",
    "        # input is of shape [batch_size]\n",
    "        # hidden is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "        # cell is of shape [n_layer * num_directions, batch_size, hidden_size]\n",
    "\n",
    "        input = input.unsqueeze(0)\n",
    "        # input shape is [1, batch_size]. reshape is needed rnn expects a rank 3 tensors as input.\n",
    "        # so reshaping to [1, batch_size] means a batch of batch_size each containing 1 index.\n",
    "\n",
    "        embedded = self.embedding(input)\n",
    "        embedded = self.dropout(embedded)\n",
    "        # embedded is of shape [1, batch_size, embedding_dim]\n",
    "\n",
    "        output, (hidden, cell) = self.rnn(embedded, (hidden, cell))\n",
    "        # generally output shape is [sequence_len, batch_size, hidden_dim * num_directions]\n",
    "        # generally hidden shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "        # generally cell shape is [num_layers * num_directions, batch_size, hidden_dim]\n",
    "\n",
    "        # sequence_len and num_directions will always be 1 in the decoder.\n",
    "        # output shape is [1, batch_size, hidden_dim]\n",
    "        # hidden shape is [num_layers, batch_size, hidden_dim]\n",
    "        # cell shape is [num_layers, batch_size, hidden_dim]\n",
    "\n",
    "        predicted = self.linear(output.squeeze(0))  # linear expects as rank 2 tensor as input\n",
    "        # predicted shape is [batch_size, output_dim]\n",
    "\n",
    "        return predicted, hidden, cell"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Seq2Seq(nn.Module):\n",
    "    ''' This class contains the implementation of complete sequence to sequence network.\n",
    "    It uses to encoder to produce the context vectors.\n",
    "    It uses the decoder to produce the predicted target sentence.\n",
    "    Args:\n",
    "        encoder: A Encoder class instance.\n",
    "        decoder: A Decoder class instance.\n",
    "    '''\n",
    "    def __init__(self, encoder, decoder):\n",
    "        super().__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "\n",
    "\n",
    "    def forward(self, src, trg, teacher_forcing_ratio=0.5):\n",
    "        # src is of shape [sequence_len, batch_size]\n",
    "        # trg is of shape [sequence_len, batch_size]\n",
    "        # if teacher_forcing_ratio is 0.5 we use ground-truth inputs 50% of time and 50% time we use decoder outputs.\n",
    "\n",
    "        batch_size = trg.shape[1]\n",
    "        max_len = trg.shape[0]\n",
    "        trg_vocab_size = self.decoder.output_dim\n",
    "\n",
    "        # to store the outputs of the decoder\n",
    "        #outputs = torch.zeros(max_len, batch_size, trg_vocab_size).to(device)\n",
    "        outputs = torch.zeros(max_len, batch_size, trg_vocab_size).cuda()\n",
    "        #outputs = torch.zeros(max_len, batch_size, trg_vocab_size)\n",
    "\n",
    "        # context vector, last hidden and cell state of encoder to initialize the decoder\n",
    "        hidden, cell = self.encoder(src)\n",
    "\n",
    "        # first input to the decoder is the <sos> tokens\n",
    "        input = trg[0, :]\n",
    "\n",
    "        for t in range(1, max_len):\n",
    "            output, hidden, cell = self.decoder(input, hidden, cell)\n",
    "            outputs[t] = output\n",
    "            use_teacher_force = random.random() < teacher_forcing_ratio\n",
    "            top1 = output.max(1)[1]\n",
    "            input = (trg[t] if use_teacher_force else top1)\n",
    "\n",
    "        # outputs is of shape [sequence_len, batch_size, output_dim]\n",
    "        return outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "INPUT_DIM = len(SRC.vocab)\n",
    "OUTPUT_DIM = len(TRG.vocab)\n",
    "ENC_EMB_DIM = 256   # encoder embedding size\n",
    "DEC_EMB_DIM = 256   # decoder embedding size (can be different from encoder embedding size)\n",
    "HID_DIM = 512       # hidden dimension (must be same for encoder & decoder)\n",
    "N_LAYERS = 2        # number of rnn layers (must be same for encoder & decoder)\n",
    "ENC_DROPOUT = 0.5   # encoder dropout\n",
    "DEC_DROPOUT = 0.5   # decoder dropout (can be different from encoder droput)\n",
    "\n",
    "# encoder\n",
    "enc = Encoder(INPUT_DIM, ENC_EMB_DIM, HID_DIM, N_LAYERS, ENC_DROPOUT)\n",
    "# decoder\n",
    "dec = Decoder(DEC_EMB_DIM, OUTPUT_DIM, HID_DIM, N_LAYERS, DEC_DROPOUT)\n",
    "# model\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = Seq2Seq(enc, dec).cuda()\n",
    "#model = Seq2Seq(enc, dec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "pad_idx = TRG.vocab.stoi['<pad>']\n",
    "# loss function calculates the average loss per token\n",
    "# passing the <pad> token to ignore_idx argument, we will ignore loss whenever the target token is <pad>\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, iterator, optimizer, criterion, clip):\n",
    "    ''' Training loop for the model to train.\n",
    "    Args:\n",
    "        model: A Seq2Seq model instance.\n",
    "        iterator: A DataIterator to read the data.\n",
    "        optimizer: Optimizer for the model.\n",
    "        criterion: loss criterion.\n",
    "        clip: gradient clip value.\n",
    "    Returns:\n",
    "        epoch_loss: Average loss of the epoch.\n",
    "    '''\n",
    "    #  some layers have different behavior during train/and evaluation (like BatchNorm, Dropout) so setting it matters.\n",
    "    model.train()\n",
    "    # loss\n",
    "    epoch_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(iterator):\n",
    "        src = batch.src\n",
    "        trg = batch.trg\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # trg is of shape [sequence_len, batch_size]\n",
    "        # output is of shape [sequence_len, batch_size, output_dim]\n",
    "        output = model(src, trg)\n",
    "\n",
    "        # loss function works only 2d logits, 1d targets\n",
    "        # so flatten the trg, output tensors. Ignore the <sos> token\n",
    "        # trg shape shape should be [(sequence_len - 1) * batch_size]\n",
    "        # output shape should be [(sequence_len - 1) * batch_size, output_dim]\n",
    "        loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "\n",
    "        # clip the gradients\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "\n",
    "        # update the parameters\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "\n",
    "    # return the average loss\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, iterator, criterion):\n",
    "    ''' Evaluation loop for the model to evaluate.\n",
    "    Args:\n",
    "        model: A Seq2Seq model instance.\n",
    "        iterator: A DataIterator to read the data.\n",
    "        criterion: loss criterion.\n",
    "    Returns:\n",
    "        epoch_loss: Average loss of the epoch.\n",
    "    '''\n",
    "    #  some layers have different behavior during train/and evaluation (like BatchNorm, Dropout) so setting it matters.\n",
    "    model.eval()\n",
    "    # loss\n",
    "    epoch_loss = 0\n",
    "\n",
    "    # we don't need to update the model pparameters. only forward pass.\n",
    "    with torch.no_grad():\n",
    "        for i, batch in enumerate(iterator):\n",
    "            src = batch.src\n",
    "            trg = batch.trg\n",
    "\n",
    "            output = model(src, trg, 0)     # turn off the teacher forcing\n",
    "\n",
    "            # loss function works only 2d logits, 1d targets\n",
    "            # so flatten the trg, output tensors. Ignore the <sos> token\n",
    "            # trg shape shape should be [(sequence_len - 1) * batch_size]\n",
    "            # output shape should be [(sequence_len - 1) * batch_size, output_dim]\n",
    "            loss = criterion(output[1:].view(-1, output.shape[2]), trg[1:].view(-1))\n",
    "\n",
    "            epoch_loss += loss.item()\n",
    "    return epoch_loss / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Epoch: 001 | Train Loss: 4.637 | Train PPL: 103.245 | Val. Loss: 4.578 | Val. PPL:  97.277 |\n",
      "| Epoch: 002 | Train Loss: 4.085 | Train PPL:  59.427 | Val. Loss: 4.349 | Val. PPL:  77.371 |\n",
      "| Epoch: 003 | Train Loss: 3.775 | Train PPL:  43.579 | Val. Loss: 4.098 | Val. PPL:  60.247 |\n",
      "| Epoch: 004 | Train Loss: 3.526 | Train PPL:  34.004 | Val. Loss: 3.995 | Val. PPL:  54.343 |\n",
      "| Epoch: 005 | Train Loss: 3.345 | Train PPL:  28.349 | Val. Loss: 3.880 | Val. PPL:  48.430 |\n",
      "| Epoch: 006 | Train Loss: 3.186 | Train PPL:  24.184 | Val. Loss: 3.824 | Val. PPL:  45.783 |\n",
      "| Epoch: 007 | Train Loss: 3.049 | Train PPL:  21.101 | Val. Loss: 3.756 | Val. PPL:  42.778 |\n",
      "| Epoch: 008 | Train Loss: 2.948 | Train PPL:  19.068 | Val. Loss: 3.682 | Val. PPL:  39.733 |\n",
      "| Epoch: 009 | Train Loss: 2.837 | Train PPL:  17.071 | Val. Loss: 3.674 | Val. PPL:  39.419 |\n",
      "| Epoch: 013 | Train Loss: 2.526 | Train PPL:  12.502 | Val. Loss: 3.631 | Val. PPL:  37.734 |\n"
     ]
    }
   ],
   "source": [
    "N_EPOCHS = 20           # number of epochs\n",
    "CLIP = 10               # gradient clip value\n",
    "SAVE_DIR = 'models'     # directory name to save the models.\n",
    "MODEL_SAVE_PATH = os.path.join(SAVE_DIR, 'seq2seq_model.pt')\n",
    "\n",
    "best_validation_loss = float('inf')\n",
    "\n",
    "if not os.path.isdir(f'{SAVE_DIR}'):\n",
    "    os.makedirs(f'{SAVE_DIR}')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "   \n",
    "    train_loss = train(model, train_iterator, optimizer, criterion, CLIP)\n",
    "    valid_loss = evaluate(model, valid_iterator, criterion)\n",
    "\n",
    "    if valid_loss < best_validation_loss:\n",
    "        best_validation_loss = valid_loss\n",
    "        torch.save(model.state_dict(), MODEL_SAVE_PATH)\n",
    "        print(f'| Epoch: {epoch+1:03} | Train Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f} | Val. Loss: {valid_loss:.3f} | Val. PPL: {math.exp(valid_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "| Test Loss: 3.634 | Test PPL:  37.855 |\n"
     ]
    }
   ],
   "source": [
    "# load the parameters(state_dict) that gave the best validation loss and run the model to test.\n",
    "model.load_state_dict(torch.load(MODEL_SAVE_PATH))\n",
    "test_loss = evaluate(model, test_iterator, criterion)\n",
    "print(f'| Test Loss: {test_loss:.3f} | Test PPL: {math.exp(test_loss):7.3f} |')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# experiment result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CLIP = 10, layer = 2\n",
    "Epoch: 013 | Train Loss: 2.526 | Train PPL:  12.502 | Val. Loss: 3.631 | Val. PPL:  37.734 |\n",
    "### CLIP = 01, layer = 2\n",
    "Epoch: 013 | Train Loss: 2.644 | Train PPL:  14.076 | Val. Loss: 3.706 | Val. PPL:  40.687 |\n",
    "### CLIP = 01, layer = 1\n",
    "Epoch: 013 | Train Loss: 2.572 | Train PPL:  13.093 | Val. Loss: 3.712 | Val. PPL:  40.934 |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# identify whether src is in GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'torch.Tensor'>\n",
      "src       ==> \tTrue\n",
      "batch.src ==> \tTrue\n"
     ]
    }
   ],
   "source": [
    "for i, batch in enumerate(train_iterator):\n",
    "    src = batch.src\n",
    "    trg = batch.trg\n",
    "    break\n",
    "print(type(src))\n",
    "print('src       ==> \\t' + str(src.is_cuda))\n",
    "print('batch.src ==> \\t' + str(batch.src.is_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrive first item from BucketIterator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = next(enumerate(train_iterator))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(0, \n",
      "[torchtext.data.batch.Batch of size 32 from MULTI30K]\n",
      "\t[.src]:[torch.cuda.LongTensor of size 23x32 (GPU 0)]\n",
      "\t[.trg]:[torch.cuda.LongTensor of size 26x32 (GPU 0)])\n",
      "------------------------------------\n",
      "x[0]             ==> 0\n",
      "x[1].src.is_cuda ==> True\n"
     ]
    }
   ],
   "source": [
    "print(x)\n",
    "print('------------------------------------')\n",
    "print('x[0]             ==> ' + str(x[0]))\n",
    "print('x[1].src.is_cuda ==> ' + str(x[1].src.is_cuda))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# retrive each element from first BucketIterator item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "first batch length = torch.Size([23, 32])\n",
      "first element in frist batch\n",
      "tensor([   2,    4,  273, 6255,   19, 1810,   25,  177,    5,   10,   16,    8,\n",
      "           3,    1,    1,    1,    1,    1,    1,    1,    1,    1,    1],\n",
      "       device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "print('first batch length = ' + str(x[1].src.size()))\n",
    "print('first element in frist batch' )\n",
    "print(x[1].src[:,0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
